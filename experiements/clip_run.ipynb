{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, CLIPProcessor, CLIPModel, AutoProcessor, CLIPVisionModel, CLIPTextModel, CLIPVisionModelWithProjection\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\") # --> returns 512d vector\n",
    "# vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\") --> returns 768d vector\n",
    "# We use vision model followed by a linear projection \n",
    "vision_model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch32\") # --> returns 512d vector\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\") # for text tokenization\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\") # for image preprocessing\n",
    "\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\") # for image + text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create a list of images from dir\n",
    "\n",
    "image_dir = './mm_retrieval_project/images'\n",
    "images = []\n",
    "images_ids = []\n",
    "for image_name in os.listdir(image_dir):\n",
    "    if image_name.endswith('.jpg'):\n",
    "        images_ids.append(image_name.split('.')[0])\n",
    "        image = Image.open(f'{image_dir}/{image_name}')\n",
    "        images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_file = './mm_retrieval_project/document.csv'\n",
    "query_file = './mm_retrieval_project/query.csv'\n",
    "\n",
    "documents = []\n",
    "documents_dict = {}\n",
    "with open(documents_file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        documents.append(line)\n",
    "    for line in documents[1:]:\n",
    "        documents_dict.update({line[0]: line[1]})\n",
    "    \n",
    "\n",
    "queries = []\n",
    "queries_dict = {}\n",
    "with open(query_file, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for line in reader:\n",
    "        queries.append(line)\n",
    "    for line in queries[1:]:\n",
    "        queries_dict.update({line[0]: line[1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting text representations for documents and queries \n",
    "docs = [doc[1] for doc in documents[1:]]\n",
    "\n",
    "def get_text_representations(texts):\n",
    "    inputs = tokenizer(texts, padding=True, return_tensors=\"pt\", truncation=True) # Downside -> context window for text in CLIP is 77\n",
    "    outputs = text_model(**inputs)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    pooled_output = outputs.pooler_output  # pooled (EOS token) states\n",
    "    return pooled_output\n",
    "\n",
    "# Creating docs_representation var to store embeddings\n",
    "docs_representation = get_text_representations(docs)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for query_id, query in queries_dict.items():\n",
    "    # print(f'{query_id} -- {query}')\n",
    "    query_represtation = get_text_representations(query)\n",
    "\n",
    "    # Computing matrix product for represntations\n",
    "    document_scores = (docs_representation @ query_represtation.T).tolist()\n",
    "    max_idx = document_scores.index(max(document_scores)) + 1\n",
    "    \n",
    "    # print(f'{documents[max_idx][0]} -- {documents[max_idx][1]}')\n",
    "    predictions.append([query_id, documents[max_idx][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting represetations for images \n",
    "import time\n",
    "top_k_images = 5\n",
    "\n",
    "def get_image_representations(images):\n",
    "    inputs = processor(images=images, return_tensors=\"pt\")\n",
    "    outputs = vision_model(**inputs)\n",
    "    image_embeds = outputs.image_embeds\n",
    "    return image_embeds\n",
    "\n",
    "image_representation = get_image_representations(images)\n",
    "\n",
    "query_image_pairs = []\n",
    "for query_id, query in queries_dict.items():\n",
    "    # print(f'{query_id} -- {query}')\n",
    "    query_represtation = get_text_representations(query)\n",
    "\n",
    "    # Computing matrix product for represntations\n",
    "    image_scores = (image_representation @ query_represtation.T).tolist()\n",
    "    indexed_scores = list(enumerate(image_scores))\n",
    "    sorted_scores = sorted(indexed_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_k_indices = [index for index, _ in sorted_scores[:top_k_images]]    \n",
    "    top_k_similar_images = [images_ids[idx] for idx in top_k_indices]\n",
    "    \n",
    "    query_image_pairs.append([query_id, top_k_similar_images])\n",
    "    # print(f'{documents[max_idx][0]} -- {documents[max_idx][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "query_image_pairs = []\n",
    "for query_id, query in queries_dict.items():\n",
    "\n",
    "    query_represtation = get_text_representations(query)\n",
    "\n",
    "    # Computing matrix product for represntations\n",
    "    image_scores = (image_representation @  query_represtation.T).tolist()\n",
    "    indexed_scores = list(enumerate(image_scores))\n",
    "    sorted_scores = sorted(indexed_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_k_indices = [index for index, _ in sorted_scores[:top_k_images]]    \n",
    "    top_k_similar_images = [images_ids[idx] for idx in top_k_indices]\n",
    "\n",
    "    query_image_pairs.append([query_id, top_k_similar_images])\n",
    "\n",
    "    # for imm in top_k_similar_images:\n",
    "    #     image = Image.open(f'{image_dir}/{imm}.jpg')\n",
    "    #     display(image)\n",
    "\n",
    "    # print(f'{documents[max_idx][0]} -- {documents[max_idx][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(predictions)):\n",
    "    predictions[idx] += query_image_pairs[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./mm_retrieval_project/clip-predictions.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['qid', 'doc_id', 'image_ids'])\n",
    "    writer.writerows(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_keys(['input_ids', 'attention_mask', 'pixel_values'])\n",
    "# dict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lineage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
